{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.12.1\n",
      "Torchvision Version:  0.13.1\n",
      "Using: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function \n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "from Data.DataLoader import *\n",
    "from models.Models import *\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Detect if we have a GPU available\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using:\",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global variables\n",
    "normalize = transforms.Normalize(\n",
    "   mean=[0.485, 0.456, 0.406],\n",
    "   std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "\n",
    "resize_size=256\n",
    "input_size = 224\n",
    "\n",
    "    \n",
    "#Transformation 1 (Basic)\n",
    "data_transforms1 = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(resize_size), #Establim la mida a 224\n",
    "        transforms.CenterCrop(input_size), #Centrem el tallat de les imatges\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(resize_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(resize_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ]),\n",
    "}\n",
    "\n",
    "\n",
    "#Transformation 2 (Grey Scale)\n",
    "data_transforms2 = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(resize_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.Grayscale(3), #Output channels 3, RGB but in gray scale\n",
    "        transforms.ToTensor()\n",
    "\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(resize_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(resize_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ]),\n",
    "}\n",
    "\n",
    "#Transformation 3 (Complete one)\n",
    "data_transforms3 = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(resize_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.RandomVerticalFlip(p=0.5), #Girem verticalment\n",
    "        transforms.RandomHorizontalFlip(p=0.5), #Girem horitzontalment\n",
    "        transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)), #Apliquem un blur\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(resize_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(resize_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ]),\n",
    "}\n",
    "\n",
    "#Transformation 4 (jitter)\n",
    "data_transforms4 = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(resize_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ColorJitter(brightness=.5, hue=.3), #Jitter color\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(resize_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(resize_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ]),\n",
    "}\n",
    "\n",
    "#transform 5\n",
    "\n",
    "channel_mean = torch.Tensor([0.485, 0.456, 0.406])\n",
    "channel_std = torch.Tensor([0.229, 0.224, 0.225])\n",
    "\n",
    "data_transforms5={\n",
    "    'train': transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "\n",
    "    transforms.RandomHorizontalFlip(p=0.6),\n",
    "    transforms.RandomRotation(degrees=(30)),\n",
    "\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=channel_mean, std=channel_std),\n",
    "]),\n",
    "    'val': transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=channel_mean, std=channel_std),\n",
    "])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "batch_size=12\n",
    "num_workers=8\n",
    "DataLoader=DogsDatasetDataloders(data_transforms4, batch_size, num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': <torch.utils.data.dataloader.DataLoader at 0x271e4a67bb0>,\n",
       " 'val': <torch.utils.data.dataloader.DataLoader at 0x271d88c96a0>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    acc_history = {\"train\": [], \"val\": []}\n",
    "    losses = {\"train\": [], \"val\": []}\n",
    "\n",
    "    # we will keep a copy of the best weights so far according to validation accuracy\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    losses[phase].append(loss.item())\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "            acc_history[phase].append(epoch_acc.item())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, acc_history, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regnet 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of classes in the dataset\n",
    "num_classes = 120\n",
    "\n",
    "# Initialize the model\n",
    "model, input_size = initialize_model_regnet_x_16(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the model to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Number of epochs to train for \n",
    "num_epochs = 15\n",
    "\n",
    "#optimizer_ft = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train and evaluate\n",
    "#model1, hist1, losses1 = train_model(model, DataLoader, criterion, optimizer_ft, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(output, label):\n",
    "    output = output.to(\"cpu\")\n",
    "    label = label.to(\"cpu\")\n",
    "\n",
    "    sm = F.softmax(output, dim=1)\n",
    "    _, index = torch.max(sm, dim=1)\n",
    "    return torch.sum((label == index)) / label.size()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    total_acc = 0.0\n",
    "\n",
    "    for batch_idx, (batch_img, batch_label) in enumerate(dataloader):\n",
    "\n",
    "        batch_img = batch_img.to(device)\n",
    "        batch_label = batch_label.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_img)\n",
    "        loss = criterion(output, batch_label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        acc = get_accuracy(output, batch_label)\n",
    "        running_acc += acc\n",
    "        total_acc += acc\n",
    "\n",
    "        if batch_idx % 100 == 0 and batch_idx != 0:\n",
    "            print(f\"[step: {batch_idx:4d}/{len(dataloader)}] loss: {running_loss / 100:.3f}\")\n",
    "            running_loss = 0.0\n",
    "            running_acc = 0.0\n",
    "    \n",
    "    return total_loss / len(dataloader), total_acc / len(dataloader)\n",
    "\n",
    "def validate(model, dataloader):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "\n",
    "    for batch_idx, (batch_img, batch_label) in enumerate(dataloader):\n",
    "\n",
    "        batch_img = batch_img.to(device)\n",
    "        batch_label = batch_label.to(device)\n",
    "\n",
    "        # optimizer.zero_grad()\n",
    "        output = model(batch_img)\n",
    "        loss = criterion(output, batch_label)\n",
    "        # loss.backward()\n",
    "        # optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        acc = get_accuracy(output, batch_label)\n",
    "        total_acc += acc\n",
    "    \n",
    "    return total_loss / len(dataloader), total_acc / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[step:  100/597] loss: 7.588\n",
      "[step:  200/597] loss: 4.480\n",
      "[step:  300/597] loss: 3.923\n",
      "[step:  400/597] loss: 4.328\n",
      "[step:  500/597] loss: 3.874\n",
      "Epoch:  0, training loss: 4.671, training acc: 0.345 validation loss: 2.046, validation acc: 0.655\n",
      "[step:  100/597] loss: 2.965\n",
      "[step:  200/597] loss: 3.074\n",
      "[step:  300/597] loss: 3.584\n",
      "[step:  400/597] loss: 3.705\n",
      "[step:  500/597] loss: 3.358\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "train_loss_history = []\n",
    "valid_loss_history = []\n",
    "\n",
    "train_acc_history = []\n",
    "valid_acc_history = []\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, DataLoader[\"train\"])\n",
    "    valid_loss, valid_acc = validate(model, DataLoader[\"val\"])\n",
    "    print(f\"Epoch: {epoch:2d}, training loss: {train_loss:.3f}, training acc: {train_acc:.3f} validation loss: {valid_loss:.3f}, validation acc: {valid_acc:.3f}\")\n",
    "\n",
    "    train_loss_history.append(train_loss)\n",
    "    valid_loss_history.append(valid_loss)\n",
    "\n",
    "    train_acc_history.append(train_acc)\n",
    "    valid_acc_history.append(valid_acc)\n",
    "\n",
    "    if valid_loss <= min(valid_loss_history):\n",
    "        torch.save(model.state_dict(), (os.getcwd()+\"/models/RegNet_X_16GfFeatureExtractionTransform4_Adam\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model1.state_dict(),(os.getcwd()+\"/models/ViTFeatureExtractTransform4\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the losses and accuracies\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "ax1.plot(losses1[\"train\"], label=\"training loss\")\n",
    "ax1.plot(losses1[\"val\"], label=\"validation loss\")\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(hist1[\"train\"],label=\"training accuracy\")\n",
    "ax2.plot(hist1[\"val\"],label=\"val accuracy\")\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvNet HAY Q PASARLE LOS INPUUUUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of classes in the dataset\n",
    "num_classes = 120\n",
    "\n",
    "# Initialize the model\n",
    "model, input_size = ConvNet(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the model to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Number of epochs to train for \n",
    "num_epochs = 15\n",
    "\n",
    "#optimizer_ft = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train and evaluate\n",
    "#model1, hist1, losses1 = train_model(model, DataLoader, criterion, optimizer_ft, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "train_loss_history = []\n",
    "valid_loss_history = []\n",
    "\n",
    "train_acc_history = []\n",
    "valid_acc_history = []\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, DataLoader[\"train\"])\n",
    "    valid_loss, valid_acc = validate(model, DataLoader[\"val\"])\n",
    "    print(f\"Epoch: {epoch:2d}, training loss: {train_loss:.3f}, training acc: {train_acc:.3f} validation loss: {valid_loss:.3f}, validation acc: {valid_acc:.3f}\")\n",
    "\n",
    "    train_loss_history.append(train_loss)\n",
    "    valid_loss_history.append(valid_loss)\n",
    "\n",
    "    train_acc_history.append(train_acc)\n",
    "    valid_acc_history.append(valid_acc)\n",
    "\n",
    "    if valid_loss <= min(valid_loss_history):\n",
    "        torch.save(model.state_dict(), (os.getcwd()+\"/models/RegNet_X_16GfFeatureExtractionTransform4_Adam\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model1.state_dict(),(os.getcwd()+\"/models/ViTFeatureExtractTransform4\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the losses and accuracies\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "ax1.plot(losses1[\"train\"], label=\"training loss\")\n",
    "ax1.plot(losses1[\"val\"], label=\"validation loss\")\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(hist1[\"train\"],label=\"training accuracy\")\n",
    "ax2.plot(hist1[\"val\"],label=\"val accuracy\")\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Net HAY Q PASARLE LOS INPUUUUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of classes in the dataset\n",
    "num_classes = 120\n",
    "\n",
    "# Initialize the model\n",
    "model, input_size = Net(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the model to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Number of epochs to train for \n",
    "num_epochs = 15\n",
    "\n",
    "#optimizer_ft = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train and evaluate\n",
    "#model1, hist1, losses1 = train_model(model, DataLoader, criterion, optimizer_ft, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "train_loss_history = []\n",
    "valid_loss_history = []\n",
    "\n",
    "train_acc_history = []\n",
    "valid_acc_history = []\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, DataLoader[\"train\"])\n",
    "    valid_loss, valid_acc = validate(model, DataLoader[\"val\"])\n",
    "    print(f\"Epoch: {epoch:2d}, training loss: {train_loss:.3f}, training acc: {train_acc:.3f} validation loss: {valid_loss:.3f}, validation acc: {valid_acc:.3f}\")\n",
    "\n",
    "    train_loss_history.append(train_loss)\n",
    "    valid_loss_history.append(valid_loss)\n",
    "\n",
    "    train_acc_history.append(train_acc)\n",
    "    valid_acc_history.append(valid_acc)\n",
    "\n",
    "    if valid_loss <= min(valid_loss_history):\n",
    "        torch.save(model.state_dict(), (os.getcwd()+\"/models/RegNet_X_16GfFeatureExtractionTransform4_Adam\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model1.state_dict(),(os.getcwd()+\"/models/ViTFeatureExtractTransform4\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the losses and accuracies\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "ax1.plot(losses1[\"train\"], label=\"training loss\")\n",
    "ax1.plot(losses1[\"val\"], label=\"validation loss\")\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(hist1[\"train\"],label=\"training accuracy\")\n",
    "ax2.plot(hist1[\"val\"],label=\"val accuracy\")\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet 50 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of classes in the dataset\n",
    "num_classes = 120\n",
    "\n",
    "# Initialize the model\n",
    "res = torchvision.models.resnet50(pretrained=True)\n",
    "for param in res.parameters():\n",
    "    param.requires_grad=False\n",
    "\n",
    "model_final = ResNet50(base_model=res, base_out_features=res.fc.out_features, num_classes=120)\n",
    "model_final = model_final.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the model to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Number of epochs to train for \n",
    "num_epochs = 15\n",
    "\n",
    "#optimizer_ft = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train and evaluate\n",
    "#model1, hist1, losses1 = train_model(model, DataLoader, criterion, optimizer_ft, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "train_loss_history = []\n",
    "valid_loss_history = []\n",
    "\n",
    "train_acc_history = []\n",
    "valid_acc_history = []\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, DataLoader[\"train\"])\n",
    "    valid_loss, valid_acc = validate(model, DataLoader[\"val\"])\n",
    "    print(f\"Epoch: {epoch:2d}, training loss: {train_loss:.3f}, training acc: {train_acc:.3f} validation loss: {valid_loss:.3f}, validation acc: {valid_acc:.3f}\")\n",
    "\n",
    "    train_loss_history.append(train_loss)\n",
    "    valid_loss_history.append(valid_loss)\n",
    "\n",
    "    train_acc_history.append(train_acc)\n",
    "    valid_acc_history.append(valid_acc)\n",
    "\n",
    "    if valid_loss <= min(valid_loss_history):\n",
    "        torch.save(model.state_dict(), (os.getcwd()+\"/models/RegNet_X_16GfFeatureExtractionTransform4_Adam\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model1.state_dict(),(os.getcwd()+\"/models/ViTFeatureExtractTransform4\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the losses and accuracies\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "ax1.plot(losses1[\"train\"], label=\"training loss\")\n",
    "ax1.plot(losses1[\"val\"], label=\"validation loss\")\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(hist1[\"train\"],label=\"training accuracy\")\n",
    "ax2.plot(hist1[\"val\"],label=\"val accuracy\")\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XV3V2N HAY Q PASARLE LOS INPUUUUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of classes in the dataset\n",
    "num_classes = 120\n",
    "\n",
    "# Initialize the model\n",
    "model, input_size = XV3V2N(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the model to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Number of epochs to train for \n",
    "num_epochs = 15\n",
    "\n",
    "#optimizer_ft = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train and evaluate\n",
    "#model1, hist1, losses1 = train_model(model, DataLoader, criterion, optimizer_ft, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "train_loss_history = []\n",
    "valid_loss_history = []\n",
    "\n",
    "train_acc_history = []\n",
    "valid_acc_history = []\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, DataLoader[\"train\"])\n",
    "    valid_loss, valid_acc = validate(model, DataLoader[\"val\"])\n",
    "    print(f\"Epoch: {epoch:2d}, training loss: {train_loss:.3f}, training acc: {train_acc:.3f} validation loss: {valid_loss:.3f}, validation acc: {valid_acc:.3f}\")\n",
    "\n",
    "    train_loss_history.append(train_loss)\n",
    "    valid_loss_history.append(valid_loss)\n",
    "\n",
    "    train_acc_history.append(train_acc)\n",
    "    valid_acc_history.append(valid_acc)\n",
    "\n",
    "    if valid_loss <= min(valid_loss_history):\n",
    "        torch.save(model.state_dict(), (os.getcwd()+\"/models/RegNet_X_16GfFeatureExtractionTransform4_Adam\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model1.state_dict(),(os.getcwd()+\"/models/ViTFeatureExtractTransform4\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the losses and accuracies\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "ax1.plot(losses1[\"train\"], label=\"training loss\")\n",
    "ax1.plot(losses1[\"val\"], label=\"validation loss\")\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(hist1[\"train\"],label=\"training accuracy\")\n",
    "ax2.plot(hist1[\"val\"],label=\"val accuracy\")\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConcatIXIRN HAY Q PASARLE LOS INPUUUUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of classes in the dataset\n",
    "num_classes = 120\n",
    "\n",
    "# Initialize the model\n",
    "model, input_size = ConcatIXIRN(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the model to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# Setup the loss fxn\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Number of epochs to train for \n",
    "num_epochs = 15\n",
    "\n",
    "#optimizer_ft = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train and evaluate\n",
    "#model1, hist1, losses1 = train_model(model, DataLoader, criterion, optimizer_ft, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "train_loss_history = []\n",
    "valid_loss_history = []\n",
    "\n",
    "train_acc_history = []\n",
    "valid_acc_history = []\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, DataLoader[\"train\"])\n",
    "    valid_loss, valid_acc = validate(model, DataLoader[\"val\"])\n",
    "    print(f\"Epoch: {epoch:2d}, training loss: {train_loss:.3f}, training acc: {train_acc:.3f} validation loss: {valid_loss:.3f}, validation acc: {valid_acc:.3f}\")\n",
    "\n",
    "    train_loss_history.append(train_loss)\n",
    "    valid_loss_history.append(valid_loss)\n",
    "\n",
    "    train_acc_history.append(train_acc)\n",
    "    valid_acc_history.append(valid_acc)\n",
    "\n",
    "    if valid_loss <= min(valid_loss_history):\n",
    "        torch.save(model.state_dict(), (os.getcwd()+\"/models/RegNet_X_16GfFeatureExtractionTransform4_Adam\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model1.state_dict(),(os.getcwd()+\"/models/ViTFeatureExtractTransform4\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the losses and accuracies\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "ax1.plot(losses1[\"train\"], label=\"training loss\")\n",
    "ax1.plot(losses1[\"val\"], label=\"validation loss\")\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(hist1[\"train\"],label=\"training accuracy\")\n",
    "ax2.plot(hist1[\"val\"],label=\"val accuracy\")\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
